var documenterSearchIndex = {"docs":
[{"location":"api/","page":"api","title":"api","text":"Modules = [ParameterSpacePartitions]\nOrder   = [:type, :function]\nPrivate = false","category":"page"},{"location":"api/#ParameterSpacePartitions.Options","page":"api","title":"ParameterSpacePartitions.Options","text":"Options(;\n    radius = .1, \n    bounds, \n    n_iters, \n    init_parms,\n    parallel = false,\n    adapt_radius! = adapt!,\n    kwargs...\n)\n\nAn object that holds configuration options for the MCMC sampler. \n\nFields\n\nparallel=false: runs model on threads if true. A speed up is observed if the evaluation \n\ntime of the function is 1 ms or greater. \n\nradius = .10: the initial radius length for each chain \nbounds: a vector of tuples representing (lowerbound, upperbound) for each dimension in \n\nthe parameter space\n\nx_range: the range of allowable values for each parameter\nn_iters: number of iterations to perform \np_eval: the function that evalues the model and pattern functions\nadapt_radius!=adapt!: a function in the form of func(chain, options; kwargs...) that adapts \n\nthe radius. \n\ninit_parms: a vector of starting points, such as [[.3,.4],[.3,.5]] in the 2 dimensional case.\nn_dims: number of dimensions in parameter space \nparm_names: a vector of symbols corresponding to parameter names. The default is [:p1,:p2,..:pn] \nadd_iters: the number of trials to run after merging chains with the same pattern located in the same region \n\n\n\n\n\n","category":"type"},{"location":"api/#ParameterSpacePartitions.adapt!-Tuple{Any, Any}","page":"api","title":"ParameterSpacePartitions.adapt!","text":"adapt!(\n    chain, \n    options; \n    t_rate = .20, \n    kwargs...\n)\n\nIteratively adapts the radius to achieve a target acceptance rate. The radius is adjusted according to the following factor c:\n\nc = exp(λ * d_rate)\n\nwhere λ is the adaption rate, and d_rate is the difference between the acceptance rate and target  acceptance rate.\n\nArguments\n\nchain: a chain for exploring the parameter space\noptions: a set of options for configuring the algorithm\n\nKeyword Arguments\n\nt_rate = .20: target acceptance rate \nkwargs...: keyword arguments that are not processed\n\n\n\n\n\n","category":"method"},{"location":"api/#ParameterSpacePartitions.estimate_volume-Tuple{Any, Any, Any, Any, Any, Vararg{Any}}","page":"api","title":"ParameterSpacePartitions.estimate_volume","text":"estimate_volume(\n    model, \n    p_fun, \n    points, \n    target, \n    bounds,  \n    args...;\n    n_sim = 10_000,\n    kwargs...\n)\n\nEstimate volume of region with an eillipsoid and hit or miss bias adjustment. \n\nArguments\n\nmodel: a model function that returns predictions given a vector of parameters \np_fun: a function that that evaluates the qualitative data pattern \npoints: a p x n matrix of sampled points where p is the number of parameters and n is the number of samples\ntarget: the target pattern associated with the points\nbounds: a vector of tuples representing (lowerbound, upperbound) for each dimension in \nargs...: arguments passed to model and p_fun\n\nKeywords\n\nn_sim=10_000: the number of samples for hit or miss bias adjustment \nkwargs...: additional keyword arguments passed to model or p_fun\n\n\n\n\n\n","category":"method"},{"location":"api/#ParameterSpacePartitions.find_partitions-Tuple{Any, Any, Any, Vararg{Any}}","page":"api","title":"ParameterSpacePartitions.find_partitions","text":"find_partitions(model, p_fun, options, args...; show_timer=false, kwargs...)\n\nPerforms parameter space partitioning.\n\nArguments\n\nmodel: a model function that returns predictions given a vector of parameters \np_fun: a function that that evaluates the qualitative data pattern \noptions: a set of options for configuring the algorithm\nargs...: arguments passed to model and p_fun\n\nKeywords\n\nshow_timer=false: displays timer and progress bar if true\nkwargs...: keyword arguments passed to model and p_fun\n\n\n\n\n\n","category":"method"},{"location":"example1/","page":"example 1","title":"example 1","text":"using Distributions\nusing ParameterSpacePartitions\nusing ParameterSpacePartitions.TestModels\nusing Random\nusing DataFrames\nRandom.seed!(544)\n\nmodel(parms, args...; kwargs...) = parms \n\nfunction p_fun(location, hypercube::HyperCube, args...; kwargs...)\n    p_bounds = hypercube.p_bounds\n    nb = length(p_bounds)\n    nd = length(location)\n    vals = fill(-100, nd)\n    for j in 1:nd\n        for i in 1:(nb-1) \n            if (location[j] ≥ p_bounds[i]) && (location[j] ≤ p_bounds[i+1])\n                vals[j] = i \n                continue\n            end\n        end\n    end\n    return vals\nend\n\n# dimensions of the hypbercue\nn_dims = 3\n# partitions per dimension\nn_part = 2\n\n# partition boundaries\nbounds = fill((0, 1), n_dims)\np_bounds = range(0, 1, n_part + 1)\nhypercube = HyperCube(p_bounds)\n\n\n# number of starting points\nn_start = 1\n# sample function\nsample(bounds) = map(b -> rand(Uniform(b...)), bounds)\n# initial starting points\ninit_parms = map(_ -> sample(bounds), 1:n_start)\nparm_names = [:p1, :p2, :p3]\n\noptions = Options(;\n    radius = .10,\n    bounds,\n    n_iters = 1000,\n    init_parms\n)\n\nresults = find_partitions(\n    model, \n    p_fun, \n    options,\n    hypercube,\n)\n\ndf = DataFrame(results)\n\ngroups = groupby(df, :pattern)\nsummary = combine(groups, \n    :p1 => minimum, :p1 => maximum, \n    :p2 => minimum, :p2 => maximum,\n    :p3 => minimum, :p3 => maximum\n) |> sort\n\n\ngroups = groupby(df, :pattern)\n\ndf_volume = combine(\n    groups,\n    x -> estimate_volume(\n        model,\n        p_fun, \n        x,  \n        bounds,\n        hypercube; \n        parm_names,\n    )\n)\n\ndf_volume.volume = df_volume.x1 / sum(df_volume.x1)","category":"page"},{"location":"example1/#Example","page":"example 1","title":"Example","text":"","category":"section"},{"location":"example1/","page":"example 1","title":"example 1","text":"In this simple example, parameter space partitioning is applied to a cube with regions of equal volume.","category":"page"},{"location":"example1/#Load-Dependencies","page":"example 1","title":"Load Dependencies","text":"","category":"section"},{"location":"example1/","page":"example 1","title":"example 1","text":"The first step is to the load dependences into your session.","category":"page"},{"location":"example1/","page":"example 1","title":"example 1","text":"using Distributions\nusing ParameterSpacePartitions\nusing ParameterSpacePartitions.TestModels\nusing Random\nusing DataFrames\nRandom.seed!(544)","category":"page"},{"location":"example1/#Model-Function","page":"example 1","title":"Model Function","text":"","category":"section"},{"location":"example1/","page":"example 1","title":"example 1","text":"Next, we define a model that accepts a vector of parameters and returns data predictions. The function model is imported above from TestModels, but is presented below for illustration. In this simple example, the model returns the parameter inputs. In more substantive model, the returned value will be the model predictions.","category":"page"},{"location":"example1/","page":"example 1","title":"example 1","text":"model(parms, args...; kwargs...) = parms ","category":"page"},{"location":"example1/#Pattern-Function","page":"example 1","title":"Pattern Function","text":"","category":"section"},{"location":"example1/","page":"example 1","title":"example 1","text":"A second function categorizes the predicted data into a qualitative pattern. At minimum, the pattern function must recieve a data input. In this example, the pattern function p_fun is imported from TestModels. The function is presented below for illustration. p_fun requires the location (i.e. parameters) and HyperCube object, which contains  partition boundaries (which is the same for each dimension). p_fun categorizes location on each dimension and returns a vector representing a pattern. For example, the pattern [1,2,1] indicates the location is in the partition for which the x axis is 1, the y axis is 2, and the z axis is 1. ","category":"page"},{"location":"example1/","page":"example 1","title":"example 1","text":"function p_fun(location, hypercube::HyperCube, args...; kwargs...)\n    p_bounds = hypercube.p_bounds\n    nb = length(p_bounds)\n    nd = length(location)\n    vals = fill(-100, nd)\n    for j in 1:nd\n        for i in 1:(nb-1) \n            if (location[j] ≥ p_bounds[i]) && (location[j] ≤ p_bounds[i+1])\n                vals[j] = i \n                continue\n            end\n        end\n    end\n    return vals\nend","category":"page"},{"location":"example1/#Model-Configuration","page":"example 1","title":"Model Configuration","text":"","category":"section"},{"location":"example1/","page":"example 1","title":"example 1","text":"Below, we will set n_dims and n_part to create a cube with a total of 2^3 = 8 regions.","category":"page"},{"location":"example1/","page":"example 1","title":"example 1","text":"# dimensions of the hypbercue\nn_dims = 3\n# partitions per dimension\nn_part = 2","category":"page"},{"location":"example1/#Boundaries","page":"example 1","title":"Boundaries","text":"","category":"section"},{"location":"example1/","page":"example 1","title":"example 1","text":"In the code below, bounds contains the upper and lower boundaries for each dimension. In addition, we must also define a HyperCube object containing the boundaries for each partition. The boundaries are stored in a variable called p_bounds.  In this example, the partitions are equally spaced along each dimension of the unit cube. Alternatively, we could use unequal spacing to increase the difficulty of exploring the parameter space.","category":"page"},{"location":"example1/","page":"example 1","title":"example 1","text":"# partition boundaries\nbounds = fill((0, 1), n_dims)\np_bounds = range(0, 1, n_part + 1)\nhypercube = HyperCube(p_bounds)","category":"page"},{"location":"example1/#Starting-Points","page":"example 1","title":"Starting Points","text":"","category":"section"},{"location":"example1/","page":"example 1","title":"example 1","text":"The sampling algorim requires a starting point to begin exploring the parameter space. The starting points must be wrapped in a vector. The starting points are sampled uniformly within the unit cube, using bounds to ensure the starting point is within allowable ranges. Although one starting point is sufficient for the present example, seeding the sampling algorithm with multiple starting points can improve performance. ","category":"page"},{"location":"example1/","page":"example 1","title":"example 1","text":"# number of starting points\nn_start = 1\n# sample function\nsample(bounds) = map(b -> rand(Uniform(b...)), bounds)\n# initial starting points\ninit_parms = map(_ -> sample(bounds), 1:n_start)\nparm_names = [:p1,:p2,:p3]","category":"page"},{"location":"example1/#Option-Configuration","page":"example 1","title":"Option Configuration","text":"","category":"section"},{"location":"example1/","page":"example 1","title":"example 1","text":"We can configure options to define and improve the performance of the algorithm. The search radius is an important configuration. The challenge is to balance the tradeoff between exploration and exploitation. If the radius is too small, it will stay in one region (or a sub-space of a region), and will fail to find new regions. By contrast, if the radius is too large, many regions will be found, but will not be well defined. Pitt et al. noted that an acceptance rate of 20% may work well in many cases, but this is a heuristic rather than a hard requirement. The options also stores the bounds and initial parameters. Threading can be enabled by setting parallel=true. Some exploration revealed that threading becomes advantageous once the evaluation time reaches 1 millisecond or longer. Otherwise, threading overhead will reduce the speed of the algorithm. ","category":"page"},{"location":"example1/","page":"example 1","title":"example 1","text":"options = Options(;\n    radius = .10,\n    bounds,\n    n_iters = 1000,\n    init_parms\n)","category":"page"},{"location":"example1/","page":"example 1","title":"example 1","text":"It is also possible to pass a custom adaption function via the keyword adapt_radius!. By default, the adaption function adjusts the radius to achieve a 40% acceptance rate. Additional information for configuring the default adaption function can be found via the help feature:","category":"page"},{"location":"example1/","page":"example 1","title":"example 1","text":"? adapt!","category":"page"},{"location":"example1/#Find-Partitions","page":"example 1","title":"Find Partitions","text":"","category":"section"},{"location":"example1/","page":"example 1","title":"example 1","text":"Now that the requisite functions and options have been specified, we can now explore the parameter space. The function find_partitions accepts the model function, the pattern function p_fun, the options object, and additional arguments and keyword arguments for p_fun.","category":"page"},{"location":"example1/","page":"example 1","title":"example 1","text":"results = find_partitions(\n    model, \n    p_fun, \n    options,\n    hypercube,\n)\nfirst(results)","category":"page"},{"location":"example1/","page":"example 1","title":"example 1","text":"results is a vector of Results objects containing the following information:","category":"page"},{"location":"example1/","page":"example 1","title":"example 1","text":"iter: the iteration of the algorithm\nchain_id: an index of the chain, i.e. 2 is the second chain\nparms: a vector of parameters\npattern: the target pattern of the chain\nacceptance: a vector indicating whether a proposal was accepted. If accepted, parms is the accepted proposal. If not accepted, parms is the same as the previous iteration.","category":"page"},{"location":"example1/#Results","page":"example 1","title":"Results","text":"","category":"section"},{"location":"example1/","page":"example 1","title":"example 1","text":"The next code block finds the minimum and maximum of each partition.","category":"page"},{"location":"example1/","page":"example 1","title":"example 1","text":"df = DataFrame(results)\ngroups = groupby(df, :pattern)\nsummary = combine(groups, \n    :p1 => minimum, :p1 => maximum, \n    :p2 => minimum, :p2 => maximum,\n    :p3 => minimum, :p3 => maximum\n) |> sort","category":"page"},{"location":"example1/#Volume-Estimation","page":"example 1","title":"Volume Estimation","text":"","category":"section"},{"location":"example1/","page":"example 1","title":"example 1","text":"The function estimate_volume approximates the volume of each region using an ellipsoid based on the covariance of sampled points in the region. As expected, the volume percentage estimates are close to 1/8 = .125.","category":"page"},{"location":"example1/","page":"example 1","title":"example 1","text":"groups = groupby(df, :pattern)\n\ndf_volume = combine(\n    groups,\n    x -> estimate_volume(\n        model,\n        p_fun, \n        x,  \n        bounds,\n        hypercube; \n        parm_names,\n    )\n)\n\ndf_volume.volume = df_volume.x1 / sum(df_volume.x1)","category":"page"},{"location":"#ParameterSpacePartitions.jl","page":"home","title":"ParameterSpacePartitions.jl","text":"","category":"section"},{"location":"","page":"home","title":"home","text":"Parameter space partitioning is a model assessment method for mapping regions of the parameter space to qualitative data patterns. Parameter space partitioning uses MCMC sampling to explore the parameter space. Each chain searches a region of the parameter space for a specific pattern. As the space is sampled, a new chain is created for each newly discovered pattern. On each iteration, a proposal is sampled uniformly from the surface of a hyperspere with the same number of dimensions as the parameter space. ","category":"page"},{"location":"#Installation","page":"home","title":"Installation","text":"","category":"section"},{"location":"","page":"home","title":"home","text":"You can install ParameterSpacePartitions.jl from the REPL by switching to the package mode using ] and then typing:","category":"page"},{"location":"","page":"home","title":"home","text":"add ParameterSpacePartitions","category":"page"},{"location":"#Quick-Example","page":"home","title":"Quick Example","text":"","category":"section"},{"location":"","page":"home","title":"home","text":"In this quick example, we use PSP to find the regions of a 2D polytope. The polytope formed by partitioning the space into 5 regions based on 5 centroids. A point in the space belongs to the region with the closest centroid. This involves defining a Polytope type, a model method, which, in this case, simply returns the proposed point, and a pfun function which returns the pattern associated with the point. The pattern is simply the id of the region associated with the closest centroid. As expected, the PSP algorithm finds all of the regions.","category":"page"},{"location":"","page":"home","title":"home","text":"using ParameterSpacePartitions\nusing Random\nusing LinearAlgebra\nusing DataFrames\nusing Distributions\nusing StatsPlots\n\nstruct Polytope\n    location::Vector{Float64}\nend\n\nfunction p_fun(location, points::Vector{Polytope}, args...; kwargs...)\n    distances = map(p -> norm(location .- p.location), points)\n    _,idx = findmin(distances)\n    return idx\nend\n\nmodel(parms, args...; kwargs...) = parms\n\n# dimensions of the hypbercue\nn_dims = 2\n# number of partitions\nn_part = 5\n# number of starting points (only supports 1 currently)\nn_start = 1\n\n\n# partition boundaries\npolytopes = [Polytope(rand(n_dims)) for i in 1:n_part]\nbounds = fill((0, 1), n_dims)\n\nsample(bounds) = map(b -> rand(Uniform(b...)), bounds)\n\ninit_parms = map(_ -> sample(bounds), 1:n_start)\n\noptions = Options(;\n    radius = .1,\n    bounds,\n    n_iters = 500,\n    parallel = false,\n    init_parms\n)\n\ndf = find_partitions(\n    model, \n    p_fun, \n    options,\n    polytopes\n)\n\nn_patterns = length(unique(df.pattern))","category":"page"},{"location":"","page":"home","title":"home","text":"using ParameterSpacePartitions\nusing Random\nusing LinearAlgebra\nusing DataFrames\nusing Distributions\nusing StatsPlots\n\nstruct Polytope\n    location::Vector{Float64}\nend\n\nfunction p_fun(location, points::Vector{Polytope}, args...; kwargs...)\n    distances = map(p -> norm(location .- p.location), points)\n    _,idx = findmin(distances)\n    return idx\nend\n\nmodel(parms, args...; kwargs...) = parms\n\n# dimensions of the hypbercue\nn_dims = 2\n# number of partitions\nn_part = 5\n# number of starting points (only supports 1 currently)\nn_start = 1\n\n# partition boundaries\npolytopes = [Polytope(rand(n_dims)) for i in 1:n_part]\nbounds = fill((0, 1), n_dims)\n\nsample(bounds) = map(b -> rand(Uniform(b...)), bounds)\n\ninit_parms = map(_ -> sample(bounds), 1:n_start)\n\noptions = Options(;\n    radius = .1,\n    bounds,\n    n_iters = 500,\n    parallel = false,\n    init_parms\n)\n\ndf = find_partitions(\n    model, \n    p_fun, \n    options,\n    polytopes\n)\n\nn_patterns = length(unique(df.pattern))","category":"page"},{"location":"","page":"home","title":"home","text":"It is also possible to see the five regions by plotting the accepted samples as a scatter plot. ","category":"page"},{"location":"","page":"home","title":"home","text":"df_accepted = filter(x -> x.acceptance, df)\n@df df_accepted scatter(:p1, :p2, group = :pattern, leg=false, grid=false)","category":"page"},{"location":"#References","page":"home","title":"References","text":"","category":"section"},{"location":"","page":"home","title":"home","text":"Pitt, M. A., Kim, W., Navarro, D. J., & Myung, J. I. (2006). Global model analysis by parameter space partitioning. Psychological Review, 113(1), 57.","category":"page"}]
}
